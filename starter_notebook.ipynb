{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to help minimize start up difficulties, we have provided you with a basic ML workflow for this project, as well as a few possible avenues to explore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: ML Workflow for Submitting *(g,h)* pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Pip Installs and Imports\n",
    "\n",
    "We will be using a package *dill* which is a variant of *pickle*, but allows a bit more expressive byte code serialization. This package is essential to saving your *(g,h)* pairs!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dill in /Users/divyakumari/anaconda3/lib/python3.10/site-packages (0.3.6)\n",
      "Requirement already satisfied: xgboost in /Users/divyakumari/anaconda3/lib/python3.10/site-packages (1.7.5)\n",
      "Requirement already satisfied: numpy in /Users/divyakumari/anaconda3/lib/python3.10/site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in /Users/divyakumari/anaconda3/lib/python3.10/site-packages (from xgboost) (1.10.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install dill\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a non-inclusive list of packages you may find helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn import *\n",
    "import dill as pkl\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download/Load Data\n",
    "\n",
    "Navigate to the project [webpage](https://declancharrison.github.io/CIS_5230_Bias_Bounty_2023/) and click \"Download Training Data\". Extract the .zip files in the folder where this notebook is located, then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('training_data.csv') \n",
    "y_train = np.genfromtxt('training_labels.csv', delimiter=',', dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ST</th>\n",
       "      <th>AGEP</th>\n",
       "      <th>CIT</th>\n",
       "      <th>COW</th>\n",
       "      <th>DDRS</th>\n",
       "      <th>DEAR</th>\n",
       "      <th>DEYE</th>\n",
       "      <th>DOUT</th>\n",
       "      <th>DRAT</th>\n",
       "      <th>DREM</th>\n",
       "      <th>...</th>\n",
       "      <th>FER</th>\n",
       "      <th>JWTRNS</th>\n",
       "      <th>LANX</th>\n",
       "      <th>MAR</th>\n",
       "      <th>MIL</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>WKHP</th>\n",
       "      <th>OCCP</th>\n",
       "      <th>RAC1P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4020.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6230.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3602.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3255.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3645.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340129</th>\n",
       "      <td>1.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3930.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340130</th>\n",
       "      <td>12.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4700.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340131</th>\n",
       "      <td>47.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340132</th>\n",
       "      <td>48.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>7330.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340133</th>\n",
       "      <td>37.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340134 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ST  AGEP  CIT  COW  DDRS  DEAR  DEYE  DOUT  DRAT  DREM  ...  FER  \\\n",
       "0       22.0  35.0  1.0  1.0   2.0   2.0   2.0   2.0   0.0   2.0  ...  2.0   \n",
       "1        2.0  23.0  1.0  1.0   2.0   2.0   2.0   2.0   0.0   2.0  ...  0.0   \n",
       "2       24.0  71.0  1.0  1.0   2.0   2.0   2.0   2.0   0.0   2.0  ...  0.0   \n",
       "3       28.0  22.0  1.0  1.0   2.0   2.0   2.0   2.0   0.0   2.0  ...  2.0   \n",
       "4       40.0  37.0  1.0  1.0   2.0   2.0   2.0   2.0   0.0   2.0  ...  2.0   \n",
       "...      ...   ...  ...  ...   ...   ...   ...   ...   ...   ...  ...  ...   \n",
       "340129   1.0  71.0  1.0  1.0   2.0   1.0   2.0   2.0   0.0   2.0  ...  0.0   \n",
       "340130  12.0  45.0  4.0  1.0   2.0   2.0   2.0   2.0   0.0   2.0  ...  2.0   \n",
       "340131  47.0  63.0  1.0  1.0   2.0   2.0   2.0   2.0   0.0   2.0  ...  0.0   \n",
       "340132  48.0  25.0  1.0  1.0   2.0   2.0   2.0   2.0   0.0   2.0  ...  0.0   \n",
       "340133  37.0  56.0  3.0  7.0   2.0   2.0   2.0   2.0   0.0   2.0  ...  0.0   \n",
       "\n",
       "        JWTRNS  LANX  MAR  MIL  SCHL  SEX  WKHP    OCCP  RAC1P  \n",
       "0          0.0   2.0  5.0  4.0  16.0  2.0  40.0  4020.0    2.0  \n",
       "1          1.0   2.0  5.0  4.0  16.0  1.0  40.0  6230.0    1.0  \n",
       "2          1.0   2.0  2.0  4.0  19.0  2.0  36.0  3602.0    2.0  \n",
       "3          1.0   2.0  5.0  4.0  19.0  2.0  32.0  3255.0    1.0  \n",
       "4         11.0   2.0  3.0  4.0  19.0  2.0  40.0  3645.0    3.0  \n",
       "...        ...   ...  ...  ...   ...  ...   ...     ...    ...  \n",
       "340129     1.0   2.0  2.0  4.0  19.0  1.0  30.0  3930.0    1.0  \n",
       "340130     1.0   1.0  1.0  4.0  19.0  2.0  40.0  4700.0    8.0  \n",
       "340131     1.0   2.0  3.0  2.0  16.0  2.0  40.0   350.0    1.0  \n",
       "340132     0.0   2.0  5.0  4.0  21.0  1.0  40.0  7330.0    1.0  \n",
       "340133     1.0   2.0  3.0  4.0  16.0  2.0  65.0   310.0    2.0  \n",
       "\n",
       "[340134 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ST', 'AGEP', 'CIT', 'COW', 'DDRS', 'DEAR', 'DEYE', 'DOUT', 'DRAT',\n",
       "       'DREM', 'ENG', 'FER', 'JWTRNS', 'LANX', 'MAR', 'MIL', 'SCHL', 'SEX',\n",
       "       'WKHP', 'OCCP', 'RAC1P'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.14533683783449"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train[(x_train['ST'] == 36) | (x_train['ENG'] == 1) | (x_train['DEYE'] == 2) | (x_train['SEX'] == 2)]).shape[0]/(x_train.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define a (g,h) pair\n",
    "\n",
    "Below is an example of training a Decision Tree Regressor on individuals identified as white from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define group function\n",
    "def g(X):\n",
    "    return (X['ST'] == 36) | (X['ENG'] == 1) | (X['DEYE'] == 2) | (X['SEX'] == 2)\n",
    "\n",
    "# initialize ML hypothesis class\n",
    "clf = sk.ensemble.GradientBoostingRegressor(max_depth = 3, n_estimators = 1100, learning_rate= 0.5, random_state = 42)\n",
    "# clf = xgb.XGBRegressor(max_depth = 5, n_estimators = 100, learning_rate= 0.5, random_state = 42)\n",
    "\n",
    "# find group indices on data\n",
    "indices = g(x_train)\n",
    "\n",
    "# fit model specifically to group\n",
    "clf.fit(x_train[indices], y_train[indices])\n",
    "\n",
    "# define hypothesis function as bound clf.predict\n",
    "h = clf.predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Save Objects\n",
    "\n",
    "The following cell will save your group model *g* with filename *g.pkl*, and your hypothesis function *h* with filename *h.pkl*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save group function to g.pkl\n",
    "with open('g.pkl', 'wb') as file:\n",
    "    pkl.dump(g, file)\n",
    "\n",
    "# save hypothesis function to h.pkl\n",
    "with open('h.pkl', 'wb') as file:\n",
    "    pkl.dump(h, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Upload Models to Google Drive and Submit PR Request with Links\n",
    "\n",
    "Follow instructions on GitHub Repo to submit a *(g,h)* pair update request!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Reducing Workflow Time Requirements by Creating a Local PDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have probably noticed, submitting a *(g,h)* pair to the GitHub repository can take a long time depending on the current workload of the server. To approximate whether or not an update will be accepted, we have provided you the PDL architecture file and a workflow that will mimic your team's private PDL maintained by the server. \n",
    "\n",
    "**NOTE: One major caveat is the validation data this workflow uses is a cut from the training data, meaning you will want to refrain from training on it to prevent overfitting.**\n",
    "\n",
    "The way we suggest getting around this without losing data efficacy is to train a *(g,h)* pair on the subset of training data that does not include the validation set, and attempt the *(g,h)* pair update on the local PDL. If the pair is rejected, you can continue tuning hyperparameters or searching for new groups. If the pair is accepted, you can retrain a new *(g,h)* pair over ALL the training data, and submit this pair to the server for an update. This will allow you to \"squeeze all the juice\" from your training data and test potential updates much quicker.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DONT CHANGE THIS CELL ###\n",
    "# from pdl import PointerDecisionList\n",
    "\n",
    "x_train_subset, x_val, y_train_subset, y_val = sk.model_selection.train_test_split(x_train, y_train, test_size = .15, random_state = 42)\n",
    "# base_clf = sk.tree.DecisionTreeRegressor(max_depth = 1, random_state = 42)\n",
    "# base_clf.fit(x_train_subset, y_train_subset)\n",
    "# PDL = PointerDecisionList(base_clf, x_train_subset, y_train_subset, x_val, y_val, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open PDL structure\n",
    "with open('PDL/model.pkl', 'rb') as file:\n",
    "    PDL = pkl.load(file)\n",
    "\n",
    "# reload group/hypothesis functions to PDL\n",
    "PDL.reload_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train[(x_train['COW'] == 7) | (x_train['COW'] == 5) | (x_train['AGEP'] >= 60) | (x_train['DEYE'] == 1) |  (x_train['RAC1P'] == 6) | (x_train['SEX'] == 1) | (x_train['SEX'] == 2)]).shape[0]/(x_train.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your *(g,h)* pair on the subset of training data below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can put these two together to train a classifier using the whole training dataset after if it has been accepted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define group function\n",
    "def g(X):\n",
    "    return (X['RAC1P'] == 1) | (X['JWTRNS'] == 1) | (X['JWTRNS'] == 11) | (X['COW'] == 8) | (X['AGEP'] >= 75) | (X['SEX'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251689     True\n",
       "157025     True\n",
       "265797     True\n",
       "158173     True\n",
       "61427      True\n",
       "          ...  \n",
       "119879     True\n",
       "259178     True\n",
       "131932     True\n",
       "146867    False\n",
       "121958     True\n",
       "Length: 289113, dtype: bool"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find group indices on training subset\n",
    "indices = g(x_train_subset)\n",
    "\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize ML hypothesis class\n",
    "# clf = sk.ensemble.RandomForestRegressor(max_depth = 4, n_estimators = 1000, random_state = 42)\n",
    "# clf = sk.ensemble.GradientBoostingRegressor(max_depth = 7, n_estimators = 800, learning_rate= 0.1, random_state = 42)\n",
    "# clf = xgb.XGBRegressor(max_depth = 5, n_estimators = 900, learning_rate= 0.1, random_state = 42)\n",
    "clf = sk.ensemble.HistGradientBoostingRegressor(max_depth = 6, max_iter = 1200, learning_rate= 0.1, random_state = 42)\n",
    "\n",
    "# fit model specifically to group subset\n",
    "clf.fit(x_train_subset[indices], y_train_subset[indices])\n",
    "\n",
    "# define hypothesis function as bound clf.predict\n",
    "h = clf.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update Rejected!\n"
     ]
    }
   ],
   "source": [
    "# compute PDL update\n",
    "update_flag = PDL.update(g, h, x_train_subset, y_train_subset, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "if update_flag:\n",
    "\n",
    "    # recompute indices over whole training dataset\n",
    "    indices = g(x_train)\n",
    "\n",
    "    # refit classifier to full group\n",
    "    clf.fit(x_train[indices], y_train[indices])\n",
    "\n",
    "    # define hypothesis function as bound clf.predict\n",
    "    h = clf.predict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99999917, 0.99999917, 0.99999917, ..., 0.99999917, 0.99999917,\n",
       "       0.99999917], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load global predictions\n",
    "global_predictions = np.genfromtxt('training_predictions.csv', delimiter=',', dtype = float)\n",
    "\n",
    "# set value hyperparameter\n",
    "value = 45000\n",
    "\n",
    "# set epsilon hyperparameter\n",
    "epsilon = 5000\n",
    "\n",
    "# define 0,1 labels where current predictions OVERESTIMATE by at least epsilon\n",
    "binary_labels = abs(global_predictions - value) < epsilon\n",
    "\n",
    "# define group classifier class\n",
    "g_clf = xgb.XGBRegressor(max_depth = 7, n_estimators = 500, random_state = 42)\n",
    "\n",
    "# fit classifier to binary labels\n",
    "g_clf.fit(x_train, binary_labels)\n",
    "\n",
    "# define g\n",
    "g = g_clf.predict\n",
    "\n",
    "g(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = g(x_train).astype(bool)\n",
    "\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n            ...\\n            0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\\n           dtype='int64', length=289113)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m h_clf \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m, n_estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m900\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# clf = sk.ensemble.HistGradientBoostingRegressor(max_depth = 6, max_iter = 1200, learning_rate= 0.1, random_state = 42)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# indices = g(x_train_subset).astype(int)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m h_clf\u001b[38;5;241m.\u001b[39mfit(\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m, y_train[indices])\n\u001b[1;32m     11\u001b[0m h \u001b[38;5;241m=\u001b[39m h_clf\u001b[38;5;241m.\u001b[39mpredict\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:6130\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6129\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n            ...\\n            0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\\n           dtype='int64', length=289113)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# initialize ML hypothesis class\n",
    "# clf = sk.ensemble.RandomForestRegressor(max_depth = 4, n_estimators = 1000, random_state = 42)\n",
    "# clf = sk.ensemble.GradientBoostingRegressor(max_depth = 7, n_estimators = 800, learning_rate= 0.1, random_state = 42)\n",
    "h_clf = xgb.XGBRegressor(max_depth = 7, n_estimators = 900, learning_rate= 0.1, random_state = 42)\n",
    "# clf = sk.ensemble.HistGradientBoostingRegressor(max_depth = 6, max_iter = 1200, learning_rate= 0.1, random_state = 42)\n",
    "\n",
    "h_clf.fit(x_train[indices], y_train[indices])\n",
    "\n",
    "h = h_clf.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update Rejected!\n"
     ]
    }
   ],
   "source": [
    "update_flag = PDL.update(g, h, x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit *(g,h)* pair to GitHub!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: You can save your PDL but it will require that your validation set does not change! Thus, you should not change the random state used to split your training data once you create your PDL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save group function to g.pkl\n",
    "with open('g.pkl', 'wb') as file:\n",
    "    pkl.dump(g, file)\n",
    "\n",
    "# save hypothesis function to h.pkl\n",
    "with open('h.pkl', 'wb') as file:\n",
    "    pkl.dump(h, file)\n",
    "\n",
    "# save PDL\n",
    "PDL.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
